---
title: "Permutation_summarization"
author: "Guanshengrui Hao"
date: "9/24/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
## Technical Setup

Suppose $m$ gene expression levels are measured on each of $n_1$ subjects from Control group and $n_2$ subjects from Treatment group. The objective is to find the genes whose expression levels are different between two groups, i.e., to test $H_{i0} : \mu_{i1} = \mu_{i2}$ versus $H_{i1} : \mu_{i1} \neq \mu_{i2}$ simultaneously for all m genes, where $\mu_{i1}$ and $\mu_{i2}$ are population means of the $i$th gene in the two groups, respectively. Suppose the proportion of true null hypotheses in $m$ tests is $\pi_0$. All gene expression levels from Control group are randomly generated from $N(\mu_1, \sigma_1^2)$ distribution. The gene expression levels of Treatment group are randomly generated from $N(\mu_1, \sigma_1^2)$ if not differentially expressed and $N(\mu_2, \sigma_2^2)$ if differentially expressed. 

Denote the observations at the $i$th gene from the Control group as $X_{i1}, \ldots, X_{in_1}$, and that from the Treatment group as $Y_{i1}, \ldots, Y_{in_2}$. If we apply a two sample t-test for each hypothesis, then the test statistic $T$ have the following form
$$
  T_i = \dfrac{\bar{Y}_i - \bar{X}_i}{\sqrt{\frac{1}{n_1}s_X^2 + \frac{1}{n_2}s_Y^2}}.
$$
Under the null hypothesis, $T_i$ follows the t-distribution with degree of freedom (df) 
$$
\nu = \dfrac{(\frac{1}{n_1}s_X^2 + \frac{1}{n_2}s_Y^2)^2}{(\frac{1}{n_1}s_X^2)^2/(n_1 - 1) + (\frac{1}{n_2}s_Y^2)^2/(n_2 - 1)}.
$$
On the other hand, under the alternative hypothesis, $T_i$ follows the non-central t-distribution with the same df $\nu$ and the non-central parameter $\delta = \dfrac{\mu_2 - \mu_1}{\sqrt{\frac{1}{n_1}\sigma^2_1 + \frac{1}{n_2}\sigma^2_2}}$. 

If equal variances are assumed, the test statistic reduces to
$$
  T_i = \dfrac{\bar{Y}_i - \bar{X}_i}{s_p^2\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}},
$$
where $s_p^2 = \dfrac{(n_1 - 1)s_X^2 + (n_2 - 1)s_Y^2}{n_1 + n_2 - 1}$. The df reduces to $\nu = n_1 + n_2 - 2$, and the non-central paramter under the alternative hypothesis is $\delta = \dfrac{\mu_2 - \mu_1}{\sigma_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$, where $\sigma_p =\sigma_1 = \sigma_2$.

## Null Case

We are interested in the distribution of the test statistic under permutation $T_i^p$. First of all, let's consider the null case, i.e., $\pi_0 = 1$, there's no difference between Control group and Treatment group. Then, no matter how we permute the observations between two groups, the test statistic $T_i^p$ follows the same distribution as $T_i$ under the null hypothesis.

Given a simplified example where $n_1 = n_2 = 3$, $\mu_1 = 0$, $\mu_2 = 2$, $\sigma_1 = \sigma_2 = 1$,
```{r null case simulation setup, eval=TRUE, echo=TRUE}
library(ggplot2)
# Number of tests
m <- 10000
# Number of samples in each group
n <- 3
# Null proportion
pi0 <- 1
# mean of control group
mu1 <- 0
# Mean of alternative in treatment group
mu2 <- 2
# Sd of the null case
sigma1 <- 1
# Sd of the alternative case
sigma2 <- 1
# Indices of alternative tests
non_null_idx <- sample(m, m*(1 - pi0))

mu <- rep(mu1, m)
mu[non_null_idx] <- mu2

control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```

we can compare the distribution of the test statistic under permutation $T_i^p$ with that of a t-distribution with df equals to $2n - 2$ and non-central parameter equals to $0$. Here the sup-label $p$ stands for the permutation that switches one observation from each group. We can also compare the p-values evaluated on observed test statistics $T_i$, permuted test statistics $T_i^p$, and samples drawn from the null t-distribution, under null distribution. 

```{r t null case, echo=FALSE, message=FALSE, warning=FALSE}
stat <- matrix(0, nrow = ncol(perms)*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

for (i in 1:ncol(perms)) {
  
  gpa <- dat[,  perms[, i]]
  gpb <- dat[, -perms[, i]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  
}

## Find a t distribution - null case
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)
t_sam <- rbind(stat[1:(2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = t_sam, aes(stat, color = perm)) + geom_density() + xlim(-10, 10) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))


## Evaluate the two-sided p-value under null
p_val <- cbind.data.frame(2*(1 - pt(abs(t_sam$stat), df = 2*n - 2)), t_sam$perm)
colnames(p_val) <- c("p_val", "perm")

# Histogram of p-values
p_plot <- ggplot(data = p_val, aes(p_val, color = perm)) + geom_density()
p_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))
```

## Alternative Case

Secondly, let's consider the scenario when the Treatment group is fully alternative case, i.e., $\pi_0 = 0$. Using the same simulation setup that $n_1 = n_2 = 3$, $\mu_1 = 0$, $\mu_2 = 2$, $\sigma_1 = \sigma_2 = 1$, we can compare the distribution of observed test statistics $T_i$, permuted test statistics $T_i^p$ with the null t-distribution; We can also compare the p-values evaluated on the $T_i$'s, $T_i^p$'s, and a sample drawn from null t-distribution under the null hypothesis.

```{r simulation setup alternative under null, eval=TRUE, echo=TRUE}
# Number of tests
m <- 10000
# Number of samples in each group
n <- 3
# Null proportion
pi0 <- 0
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
mu2 <- 2

non_null_idx <- sample(m, m*(1 - pi0))
mu <- rep(mu1, m)
mu[non_null_idx] <- mu2
sigma1 <- 1
sigma2 <- 1
control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```


```{r t alternative case, echo=FALSE, message=FALSE, warning=FALSE}
stat <- matrix(0, nrow = ifelse(ncol(perms) > 50, 10, ncol(perms))*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

for (i in 1:ifelse(ncol(perms) > 50, 10, ncol(perms))) {
  
  gpa <- dat[,  perms[, i]]
  gpb <- dat[, -perms[, i]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  
}
## Drawn a sample from null as comparison
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)

plot_stat <- rbind.data.frame(stat[1: (2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-10, 10) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))


## Two-sided p-value
p_val <- cbind.data.frame(2*(1 - pt(abs(plot_stat$stat), df = 2*n - 2)), plot_stat$perm)
colnames(p_val) <- c("p_val", "perm")

# Histogram of p-values
p_plot <- ggplot(data = p_val, aes(p_val, color = perm)) + geom_density()
p_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))
```


## Null and alternative mixed case

We then consider the scenario when the Treatment group has null and alternative mixed together. Use the same simulation setup that $n_1 = n_2 = 3$, $\mu_1 = 0$, $\mu_2 = 2$, $\sigma_1 = \sigma_2 = 1$, with the only difference being that $\pi_0 = 0.5$. We can similarly compare the distributions of test statistics and the p-values evaluated under null.


```{r simulation setup mixed under null, eval=TRUE, echo=TRUE}
# Number of tests
m <- 10000
# Number of samples in each group
n <- 3
# Null proportion
pi0 <- 0.5
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
mu2 <- 2

non_null_idx <- sample(m, m*(1 - pi0))
mu <- rep(mu1, m)
mu[non_null_idx] <- mu2
sigma1 <- 1
sigma2 <- 1
control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```


```{r t mixed case, echo=FALSE, message=FALSE, warning=FALSE}
stat <- matrix(0, nrow = ifelse(ncol(perms) > 50, 10, ncol(perms))*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

for (i in 1:ifelse(ncol(perms) > 50, 10, ncol(perms))) {
  
  gpa <- dat[,  perms[, i]]
  gpb <- dat[, -perms[, i]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  
}
## Drawn a sample from null as comparison
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)

plot_stat <- rbind.data.frame(stat[1: (2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-10, 10) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))


## Two-sided p-value
p_val <- cbind.data.frame(2*(1 - pt(abs(plot_stat$stat), df = 2*n - 2)), plot_stat$perm)
colnames(p_val) <- c("p_val", "perm")

# Histogram of p-values
p_plot <- ggplot(data = p_val, aes(p_val, color = perm)) + geom_density()
p_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))
```


## Null and alternative mixed case as $n$ increases


Let's now consider the mixed scenario as $n$ increases, i.e., the number of samples in each group increases. Use the same simulation setup as previous one that $\mu_1 = 0$, $\mu_2 = 2$, $\sigma_1 = \sigma_2 = 1$, $\pi_0 = 0.5$, with the only difference being that $n_1 = n_2 = 5$. We can similarly compare the distributions of test statistics and the p-values evaluated under null.


```{r simulation setup mixed under null sample size 5, eval=TRUE, echo=TRUE}
# Number of tests
m <- 10000
# Number of samples in each group
n <- 5
# Null proportion
pi0 <- 0.5
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
mu2 <- 2

non_null_idx <- sample(m, m*(1 - pi0))
mu <- rep(mu1, m)
mu[non_null_idx] <- mu2
sigma1 <- 1
sigma2 <- 1
control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```


```{r t mixed case sample size 5, echo=FALSE, message=FALSE, warning=FALSE}
stat <- matrix(0, nrow = ifelse(ncol(perms) > 50, 10, ncol(perms))*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

for (i in 1:ifelse(ncol(perms) > 50, 10, ncol(perms))) {
  
  gpa <- dat[,  perms[, i]]
  gpb <- dat[, -perms[, i]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  
}
## Drawn a sample from null as comparison
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)

plot_stat <- rbind.data.frame(stat[1: (2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-10, 10) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))


## Two-sided p-value
p_val <- cbind.data.frame(2*(1 - pt(abs(plot_stat$stat), df = 2*n - 2)), plot_stat$perm)
colnames(p_val) <- c("p_val", "perm")

# Histogram of p-values
p_plot <- ggplot(data = p_val, aes(p_val, color = perm)) + geom_density()
p_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))
```

The p-values evaluated on permuted test statistic become less flat and have more values close to $0$. If we further consider $n_1 = n_2 = 10$, we have

```{r simulation setup mixed under null sample size 10, eval=TRUE, echo=TRUE}
# Number of tests
m <- 10000
# Number of samples in each group
n <- 10
# Null proportion
pi0 <- 0.5
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
mu2 <- 2

non_null_idx <- sample(m, m*(1 - pi0))
mu <- rep(mu1, m)
mu[non_null_idx] <- mu2
sigma1 <- 1
sigma2 <- 1
control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```


```{r t mixed case sample size 10, echo=FALSE, message=FALSE, warning=FALSE}
stat <- matrix(0, nrow = ifelse(ncol(perms) > 50, 10, ncol(perms))*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

for (i in 1:ifelse(ncol(perms) > 50, 10, ncol(perms))) {
  
  gpa <- dat[,  perms[, i]]
  gpb <- dat[, -perms[, i]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  
}
## Drawn a sample from null as comparison
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)

plot_stat <- rbind.data.frame(stat[1: (2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-10, 10) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))


## Two-sided p-value
p_val <- cbind.data.frame(2*(1 - pt(abs(plot_stat$stat), df = 2*n - 2)), plot_stat$perm)
colnames(p_val) <- c("p_val", "perm")

# Histogram of p-values
p_plot <- ggplot(data = p_val, aes(p_val, color = perm)) + geom_density()
p_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))
```

Now the p-values calculated from the permuted test statistics and observed test statisitics are very close. However, here the permuted test statistics are from a \textbf{single} permutation that switches only \textbf{one} sample from each group. We can choose another permutation that switches roughly $n_1/2 = n_2/2$ samples (in this case $5$) from each group and compare.

```{r simulation setup mixed under null sample size 10 central permutation, eval=TRUE, echo=TRUE}
# Number of tests
m <- 10000
# Number of samples in each group
n <- 6
# Null proportion
pi0 <- 0
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
mu2 <- 2

non_null_idx <- sample(m, m*(1 - pi0))
mu <- rep(mu1, m)
mu[non_null_idx] <- mu2
sigma1 <- 1
sigma2 <- 1
control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```


```{r t mixed case sample size 10 central permutation, echo=FALSE, message=FALSE, warning=FALSE}
stat <- matrix(0, nrow = ifelse(ncol(perms) > 50, 10, ncol(perms))*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

idx <- which(apply(perms, 2, FUN = function(x) sum(x %in% (1:n)) == floor(n/2)))[1:9]
i <- 1

for (j in c(1,idx)) {
  
  gpa <- dat[,  perms[, j]]
  gpb <- dat[, -perms[, j]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  i <- i + 1
}
## Drawn a sample from null as comparison
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)

plot_stat <- rbind.data.frame(stat[1: (2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-10, 10) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))


## Two-sided p-value
p_val <- cbind.data.frame(2*(1 - pt(abs(plot_stat$stat), df = 2*n - 2)), plot_stat$perm)
colnames(p_val) <- c("p_val", "perm")

# Histogram of p-values
p_plot <- ggplot(data = p_val, aes(p_val, color = perm)) + geom_density()
p_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))

```

We can see that as $n_1 = n_2$ increases, there is still a sub-sample of permutations that can serve as an approximation of null distribution.


## Lemma 1

Denote $T = \dfrac{\bar{Y} - \bar{X}}{\sqrt{\frac{1}{n_1}s_X^2 + \frac{1}{n_2}s_Y^2}}$ as the test statistic for the observed grouping and $T^p = \dfrac{\bar{Y^p} - \bar{X^p}}{\sqrt{\frac{1}{n_1}s_{X^p}^2 + \frac{1}{n_2}s_{Y^p}^2}}$ as that for the permuted grouping corresponding to permutation $p$. Assume $n_1 = n_2 = 2n$, $Y_i$'s and $X_i$'s are both i.i.d., respectively. Then we can show that 
$$
  \mathbb{E}(T^p) = 0,
$$
given that $p$ is a permutation that switches $n$ samples between two groups.

## Proof

Without loss of generality, consider permutation $p$ keeps the first $n$ samples within their original groups and switches the second $n$ samples across groups. As a result, we have 
$$
  \bar{Y^p} = \frac{1}{2n}\left(\sum_{i = 1}^nY_i + \sum_{i = n + 1}^{2n}X_i\right)
$$
$$
  \bar{X^p} = \frac{1}{2n}\left(\sum_{i = 1}^nX_i + \sum_{i = n + 1}^{2n}Y_i\right)
$$
For simplicity, denote $\bar{X}_1 = \frac{1}{n}\sum_{i = 1}^nX_i$ and $\bar{X}_2 = \frac{1}{n}\sum_{i = n + 1}^{2n}X_i$ as the sample mean of the first and second $n$ samples, respectively. Similarly, denote $\bar{Y}_1 = \frac{1}{n}\sum_{i = 1}^nY_i$ and $\bar{Y}_2 = \frac{1}{n}\sum_{i = n + 1}^{2n}Y_i$. Then we have
$$
  \bar{Y^p} = \frac{1}{2}(\bar{Y}_1 + \bar{X}_2)
$$
$$
  \bar{X^p} = \frac{1}{2}(\bar{X}_1 + \bar{Y}_2)
$$
With the defined notations, we can represent the sample variance as
$$
  S^2_{Y^p} = \frac{1}{2n - 1}\left[\sum_{i = 1}^{n}\left(Y_i - \frac{1}{2}(\bar{Y}_1 + \bar{X}_2)\right)^2 + \sum_{i = n + 1}^{2n}\left(X_i - \frac{1}{2}(\bar{Y}_1 + \bar{X}_2)\right)^2\right]
$$
$$
  S^2_{X^p} = \frac{1}{2n - 1}\left[\sum_{i = 1}^{n}\left(X_i - \frac{1}{2}(\bar{X}_1 + \bar{Y}_2) \right)^2 + \sum_{i = n + 1}^{2n}\left(Y_i - \frac{1}{2}(\bar{X}_1 + \bar{Y}_2) \right)^2\right]
$$
Through expansion and combining terms, we can simplify $S^2_{Y^p}$ and $S^2_{X^p}$ into
$$
  S^2_{Y^p} = \dfrac{1}{2n - 1}\left[\sum_{i = 1}^{n}(Y_i - \bar{Y}_1)^2 + \sum_{i = n + 1}^{2n}(X_i - \bar{X}_2)^2 + \frac{n}{2}(\bar{Y}_1 - \bar{X}_2)^2\right]
$$
$$
  S^2_{X^p} = \dfrac{1}{2n - 1}\left[\sum_{i = 1}^{n}(X_i - \bar{X}_1)^2 + \sum_{i = n + 1}^{2n}(Y_i - \bar{Y}_2)^2 + \frac{n}{2}(\bar{X}_1 - \bar{Y}_2)^2\right]
$$
Since $X_i$'s and $Y_i$'s are both i.i.d., respectively, we have mutual indepedence among $\bar{X}_1$, $\bar{X_2}$, $\bar{Y_1}$ and $\bar{Y_2}$. Furthermore, based on the that
$$
  \Cov(\bar{X}_1, X_i - \bar{X}_1) = 0, \forall i = 1, \ldots, n,
$$
we have
$$
  \Cov\left(\bar{X}_1, \sum_{i = 1}^n(X_i - \bar{X}_1)^2\right) = 0.
$$ 
Similarly, we have 
$$ \Cov\left(\bar{X}_2, \sum_{i = n + 1}^{2n}(X_i - \bar{X}_2)^2\right) = 0, \quad \Cov\left(\bar{Y}_1 \perp \sum_{i = 1}^n(Y_i - \bar{Y}_1)^2\right) = 0, \quad \Cov\left(\bar{Y}_2, \sum_{i = n + 1}^{2n}(Y_i - \bar{Y}_2)^2\right) = 0.
$$
To show that $\Cov\left(\bar{Y^p} - \bar{X^p}, S^2_{X^p} + S^2_{Y^p}\right) = 0$, it suffices to show $\Cov\left(\bar{Y^p} - \bar{X^p},  (\bar{X}_1 - \bar{Y}_2)^2 + (\bar{Y}_1 - \bar{X}_2)^2\right) = 0$. We have 
\begin{eqnarray*}
  &   & \Cov\left(\bar{Y^p} - \bar{X^p}, (\bar{X}_1 - \bar{Y}_2)^2 + (\bar{Y}_1 - \bar{X}_2)^2\right)\\
  & = & \frac{1}{2}\left\{\Cov\left(\bar{Y}_1, (\bar{X}_1 - \bar{Y}_2)^2 + (\bar{Y}_1 - \bar{X}_2)^2\right) - \Cov\left(\bar{X}_1, (\bar{X}_1 - \bar{Y}_2)^2 + (\bar{Y}_1 - \bar{X}_2)^2\right) \right.\\
  &   & \left.- \Cov\left(\bar{Y}_2, (\bar{X}_1 - \bar{Y}_2)^2 + (\bar{Y}_1 - \bar{X}_2)^2\right) + \Cov\left(\bar{X}_2, (\bar{X}_1 - \bar{Y}_2)^2 + (\bar{Y}_1 - \bar{X}_2)^2\right) \right\}\\
  & = & \frac{1}{2}\left(\Cov(\bar{Y}_1, \bar{Y}_1^2) - \Cov(\bar{Y}_2, \bar{Y}_2^2)\right) - \left(\Cov(\bar{Y}_1, \bar{Y}_1\bar{X}_2) - \Cov(\bar{Y}_2, \bar{Y}_2\bar{X}_1)\right)\\
  &   & - \frac{1}{2}\left(\Cov(\bar{X}_1, \bar{X}_1^2) - \Cov(\bar{X}_2, \bar{X}_2^2)\right) + \left(\Cov(\bar{X}_1, \bar{X}_1\bar{Y}_2) - \Cov(\bar{X}_2, \bar{X}_2\bar{Y}_1)\right)\\
  & = & 0
\end{eqnarray*}
Based on all the parts we have shown, we get $\Cov\left(\bar{Y^p} - \bar{X^p}, S^2_{Y^p} + S^2_{X^p}\right) = 0$. Therefore,
we have 
$$
  \mathbb{E}(T^p) = \mathbb{E}(\bar{Y^p} - \bar{X^p})\mathbb{E}\left(\dfrac{1}{\sqrt{\frac{1}{2n}s_X^2 + \frac{1}{2n}s_Y^2}} \right) = 0,
$$
which finishes the proof.

## Lemma 2
Follow the same assumption as Lemma 1 that $n_1 = n_2 = 2n$, $Y_i$'s and $X_i$'s are both normally distributed, with equal standard deviation $\sigma$, $\mathbb{E}(Y_i) = \mu_2$ and $\mathbb{E}(X_i) = \mu_1$, for all $i$. Suppose $p$ is a balanced permutation, i.e., $p$ switches $n$ samples across two groups. Then we have following result
$$
  T^p|J \sim \sqrt{\dfrac{4n - 2}{4n - 2 + 2J}}t_{4n - 2 + 2J}
$$
where $J \sim \mathrm{Pois}\left(\dfrac{n}{2}(\mu_2 - \mu_1)^2\right)$.

## Proof
From the proof of Lemma 1 we have shown that $(\bar{Y^p} - \bar{X^p}) \perp (S^2_{Y^p} + S^2_{X^p})$. Let's start with the denominator. Leave the square root and constant multiplier $1/2n$ aside,
$$
    S^2_{Y^p} + S^2_{X^p} = \dfrac{1}{2n - 1}\left[\sum_{i = 1}^{n}(Y_i - \bar{Y}_1)^2 + \sum_{i = n + 1}^{2n}(X_i - \bar{X}_2)^2 + \frac{n}{2}(\bar{Y}_1 - \bar{X}_2)^2 + \sum_{i = 1}^{n}(X_i - \bar{X}_1)^2 + \sum_{i = n + 1}^{2n}(Y_i - \bar{Y}_2)^2 + \frac{n}{2}(\bar{X}_1 - \bar{Y}_2)^2\right].
$$
We have also proved that withing the branket, all six terms are mutually independent. It's also clear that 
$$
  \sum_{i = 1}^{n}(Y_i - \bar{Y}_1)^2, \sum_{i = n + 1}^{2n}(X_i - \bar{X}_2)^2, \sum_{i = 1}^{n}(X_i - \bar{X}_1)^2, \sum_{i = n + 1}^{2n}(Y_i - \bar{Y}_2)^2 \sim \sigma^2\chi^2_{n - 1},
$$
so we need to figure out the distribution of $\frac{n}{2}(\bar{Y}_1 - \bar{X}_2)^2$ and $\frac{n}{2}(\bar{X}_1 - \bar{Y}_2)^2$. Since $\mathbb{E}\left(\dfrac{\bar{Y}_1 - \bar{X}_2}{\sqrt{2/n}}\right) = \dfrac{1}{\sqrt{2/n}}(\mu_2 - \mu_1)$ and $\Var\left(\dfrac{\bar{Y}_1 - \bar{X}_2}{\sqrt{2/n}}\right) = \sigma^2$, we have 
$$
  \dfrac{\bar{Y}_1 - \bar{X}_2}{\sigma\sqrt{2/n}} \sim N(\dfrac{1}{\sqrt{2/n}}(\mu_2 - \mu_1), 1).
$$
Therefore, we know that 
$$
  \left(\dfrac{\bar{X}_1 - \bar{Y}_2}{\sigma\sqrt{2/n}}\right) = \frac{1}{\sigma^2}\left(\frac{n}{2}(\bar{Y}_1 - \bar{X}_2)^2\right) \sim \chi_1^2\left((\dfrac{1}{\sqrt{2/n}}(\mu_2 - \mu_1))^2\right) = \chi_1^2\left( \frac{n}{2}(\mu_2 - \mu_1)^2\right),
$$
where $\chi_k^2(\lambda)$ stands for a non-central chi-square distribution with df $k$ and non-central parameter $\lambda$. Similarly,
$$
  \frac{1}{\sigma^2}\left(\frac{n}{2}(\bar{X}_1 - \bar{Y}_2)^2)^2\right) \sim \chi_1^2\left( \frac{n}{2}(\mu_2 - \mu_1)^2\right)
$$
as well. Follow the property of non-central chi-square random variables that if $V_1 \sim \chi^2_{k}(\lambda_1)$, $V_2 \sim \chi^2_{k}(\lambda_2)$, $V_1 \perp V_2$, then $V_1 + V_2 \sim \chi^2_{2k}(\lambda_1 + \lambda_2)$, we have 
$$
  \frac{1}{\sigma^2}\left(\frac{n}{2}(\bar{Y}_1 - \bar{X}_2)^2 + \frac{n}{2}(\bar{X}_1 - \bar{Y}_2)^2\right) \sim \chi^2_2\left(n(\mu_2 - \mu_1)^2\right)
$$
Another property of non-central chi-square distribution states that a non-central chi-square distribution can be treated as a Poisson weighted mixture of central chi-square distribution, i.e., 
$$
  \chi^2_k(\lambda) = \chi^2_{k + 2j}\cdot\Pr(J = j), \quad J \sim \mathrm{Pois}(\lambda/2).
$$
Therefore, we can rewrite the previous formula as
$$
  \frac{1}{\sigma^2}\left(\frac{n}{2}(\bar{Y}_1 - \bar{X}_2)^2 + \frac{n}{2}(\bar{X}_1 - \bar{Y}_2)^2\right)|J \sim \chi^2_{2 + 2J}, \quad J \sim \mathrm{Pois}\left(\frac{n}{2}(\mu_2 - \mu_1)^2\right).
$$
Combine with the other four terms in the denominator, we have
$$
  \dfrac{2n - 1}{\sigma}(S^2_{Y^p} + S^2_{X^p})|J \sim \chi^2_{4(n - 1) + (2 + 2J)} = \chi^2_{4n - 2 + 2J}, \quad J \sim \mathrm{Pois}\left(\frac{n}{2}(\mu_2 - \mu_1)^2\right).
$$
The construction of a Student-t random variable is given by
$$
  T = \dfrac{Z}{\sqrt{V/v}} \sim t_v,
$$
where $Z \sim N(0, 1)$, $V \sim \chi^2_v$ and $Z \perp V$. We have shown that the numerator and the denominator of $T^p$ are independent. We also have
$$
  \mathbb{E}(\bar{Y^p} - \bar{X^p}) = 0,
$$
$$
  \Var(\bar{Y^p} - \bar{X^p}) = \dfrac{1}{n}\sigma^2,
$$
thus
$$
  \dfrac{\bar{Y^p} - \bar{X^p}}{\sqrt{\frac{1}{n}\sigma}} \sim N(0, 1).
$$
Divide both the numerator and denominator of $T^p$ by $\sqrt{\frac{1}{n}\sigma}$,
$$
  T^p|J = \dfrac{\frac{\bar{Y^p} - \bar{X^p}}{\sqrt{\frac{1}{n}\sigma}}}{\sqrt{\dfrac{\frac{1}{2n}s_X^2 + \frac{1}{2n}s_Y^2}{\frac{1}{n}\sigma}}}|J = \dfrac{\frac{\bar{Y^p} - \bar{X^p}}{\sqrt{\frac{1}{n}\sigma}}}{\sqrt{\dfrac{(2n - 1)(S^2_{Y^p} + S^2_{X^p})}{2(2n - 1)\sigma}}}|J = \dfrac{\frac{\bar{Y^p} - \bar{X^p}}{\sqrt{\frac{1}{n}\sigma}}}{\sqrt{\dfrac{4n - 2 + 2J}{4n - 2}}\sqrt{\dfrac{(2n - 1)(S^2_{Y^p} + S^2_{X^p})}{(4n - 2 + 2J)\sigma}}}|J.
$$
Based on previously proved result that 
$$
    \dfrac{2n - 1}{\sigma}(S^2_{Y^p} + S^2_{X^p})|J \sim \chi^2_{4n - 2 + 2J}, \quad J \sim \mathrm{Pois}\left(\frac{n}{2}(\mu_2 - \mu_1)^2\right),
$$
we have 
$$
  T^p|J = \sqrt{\dfrac{4n - 2}{4n - 2 + 2J}}t_{4n - 2 + 2J}, \quad J \sim \mathrm{Pois}\left(\frac{n}{2}(\mu_2 - \mu_1)^2\right),
$$
which finishes the proof.



## Remark
Under the null hypothesis where $\mu_1 = \mu_2$, $J \sim \mathrm{Pois}(0) = 0$ with probability $1$. So $T^p \sim T^p|(J = 0)\cdot \Pr(J = 0) = T^p|(J = 0) = t_{4n - 2}$, which is aligned with previous result in null case section.

## Simulation evidence
Consider the null proportion $\pi_0 = 0$, i.e., full alternative case. Let $\mu_1 = 0$, $\mu_2 = 2$, $\sigma_1 = \sigma_2 = 1$ same as before. When $n_1 = n_2 = 4$, we can say the distribution of permuted test statistics (green line) and the distribution of samples drawn from derived Poisson mixture of t ramdom variables (purple line) match well. 
```{r simulation setup proving alternative is poisson mixture 4, eval=TRUE, echo=TRUE}
# Number of tests
m <- 10000
# Number of samples in each group
n <- 4
# Null proportion
pi0 <- 0
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
mu2 <- 2

non_null_idx <- sample(m, m*(1 - pi0))
mu <- rep(mu1, m)
mu[non_null_idx] <- mu2
sigma1 <- 1
sigma2 <- 1
control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```


```{r proving alternative is poisson mixture 4, echo=FALSE, message=FALSE, warning=FALSE}
idx <- which(apply(perms, 2, FUN = function(x) sum(x %in% (1:n)) == floor(n/2)))

idx <- c(1, idx[1:ifelse(length(idx) > 9, 9, length(idx))])

stat <- matrix(0, nrow = length(idx)*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

i <- 1

for (j in idx) {
  
  gpa <- dat[,  perms[, j]]
  gpb <- dat[, -perms[, j]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  i <- i + 1
}
## Drawn a sample from null as comparison
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)

J <- rpois(m, n/4*(mu2 - mu1)^2)
test_sam <- rt(m, df = 2*n - 2 + 2*J)
test_sam <- cbind.data.frame(sqrt((2*n - 2)/(2*n - 2 + 2*J))*test_sam, rep('test', m))
colnames(test_sam) <- colnames(t_sam)
t_sam <- rbind(t_sam, test_sam)

plot_stat <- rbind.data.frame(stat[1: (2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-10, 10) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t", "test"), labels=c("Observed", "Permuted", "Drawn from null", "Poisson Mixture"))
```

When $n_1 = n_2 = 10$, green and purple match well.

```{r simulation setup proving alternative is poisson mixture 10, eval=TRUE, echo=FALSE}
# Number of tests
m <- 10000
# Number of samples in each group
n <- 10
# Null proportion
pi0 <- 0
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
mu2 <- 2

non_null_idx <- sample(m, m*(1 - pi0))
mu <- rep(mu1, m)
mu[non_null_idx] <- mu2
sigma1 <- 1
sigma2 <- 1
control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```


```{r proving alternative is poisson mixture 10, echo=FALSE, message=FALSE, warning=FALSE}
idx <- which(apply(perms, 2, FUN = function(x) sum(x %in% (1:n)) == floor(n/2)))

idx <- c(1, idx[1:ifelse(length(idx) > 9, 9, length(idx))])

stat <- matrix(0, nrow = length(idx)*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

i <- 1

for (j in idx) {
  
  gpa <- dat[,  perms[, j]]
  gpb <- dat[, -perms[, j]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  i <- i + 1
}
## Drawn a sample from null as comparison
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)

J <- rpois(m, n/4*(mu2 - mu1)^2)
test_sam <- rt(m, df = 2*n - 2 + 2*J)
test_sam <- cbind.data.frame(sqrt((2*n - 2)/(2*n - 2 + 2*J))*test_sam, rep('test', m))
colnames(test_sam) <- colnames(t_sam)
t_sam <- rbind(t_sam, test_sam)

plot_stat <- rbind.data.frame(stat[1: (2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-10, 10) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t", "test"), labels=c("Observed", "Permuted", "Drawn from null", "Poisson Mixture"))
```

Let $\mu_2 = 3$, $\mu_1 = 0$, $n_1 \propto n_2 = 10$, green and purple also match well.

```{r simulation setup proving alternative is poisson mixture 10 mu2 3, eval=TRUE, echo=FALSE}
# Number of tests
m <- 10000
# Number of samples in each group
n <- 10
# Null proportion
pi0 <- 0
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
mu2 <- 3

non_null_idx <- sample(m, m*(1 - pi0))
mu <- rep(mu1, m)
mu[non_null_idx] <- mu2
sigma1 <- 1
sigma2 <- 1
control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```


```{r proving alternative is poisson mixture 10 mu2 3, echo=FALSE, message=FALSE, warning=FALSE}
idx <- which(apply(perms, 2, FUN = function(x) sum(x %in% (1:n)) == floor(n/2)))

idx <- c(1, idx[1:ifelse(length(idx) > 9, 9, length(idx))])

stat <- matrix(0, nrow = length(idx)*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

i <- 1

for (j in idx) {
  
  gpa <- dat[,  perms[, j]]
  gpb <- dat[, -perms[, j]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  i <- i + 1
}
## Drawn a sample from null as comparison
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)

J <- rpois(m, n/4*(mu2 - mu1)^2)
test_sam <- sqrt((2*n - 2)/(2*n - 2 + 2*J))*rt(m, df = 2*n - 2 + 2*J)
test_sam <- test_sam#*sqrt(n/4*(mu2 - mu1)^2)
test_sam <- cbind.data.frame(test_sam, rep('test', m))
colnames(test_sam) <- colnames(t_sam)
t_sam <- rbind(t_sam, test_sam)

plot_stat <- rbind.data.frame(stat[1: (2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-15, 15) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t", "test"), labels=c("Observed", "Permuted", "Drawn from null", "Poisson Mixture"))
```


```{r simulation setup proving negative binomial mixture 10, eval=TRUE, echo=FALSE}
# Number of tests
m <- 10000
# Number of samples in each group
n <- 6
# Null proportion
pi0 <- 0
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
sigma_mu <- 3
mu2 <- rnorm(m, mu1, sigma_mu)

non_null_idx <- which(abs(mu1 - mu2) > 0.5)

sigma1 <- 1
sigma2 <- 1
control <- NULL
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu2, sd = sigma2), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```

```{r proving negative mixture 10 mu2 3, echo=FALSE, message=FALSE, warning=FALSE}
idx <- which(apply(perms, 2, FUN = function(x) sum(x %in% (1:n)) == floor(n/2)))

idx <- c(1, idx[1:ifelse(length(idx) > 9, 9, length(idx))])

stat <- matrix(0, nrow = length(idx)*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

i <- 1

for (j in idx) {
  
  gpa <- dat[,  perms[, j]]
  gpb <- dat[, -perms[, j]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  i <- i + 1
}

# stat$perm[stat$perm != "1"] <- "permuted"
## Drawn a sample from null as comparison
# t_sam <- rt(m, df = 2*n - 2)
# t_sam <- cbind.data.frame(t_sam, rep('t', m))
# colnames(t_sam) <- colnames(stat)
M <- 5*m
J <- rnbinom(M, size = 0.5, prob = 1/(1 + n*sigma_mu^2/2))
#J <- rpois(m, n/4*(mu2 - mu1)^2)
test_sam <- sqrt((2*n - 2)/(2*n - 2 + 2*J))*rt(M, df = 2*n - 2 + 2*J)
test_sam <- test_sam#*sqrt(n/4*(mu2 - mu1)^2)
test_sam <- cbind.data.frame(test_sam, rep('test', M))
colnames(test_sam) <- colnames(stat)

plot_stat <- rbind.data.frame(stat[1:(2*m),], test_sam)
true_stat <- stat[1:m, ][-non_null_idx, ]
true_stat$perm <- "null"
plot_stat <- rbind.data.frame(plot_stat, true_stat)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-15, 15) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "test", "null"), labels=c("Observed", "Permuted", "T Mixture", "Null Stat"))
```






























```{r simulation setup, eval=FALSE, echo=FALSE}
library(ggplot2)
# Number of tests
m <- 10000
# Number of samples in each group
n <- 10
# Null proportion
pi0 <- 0.5
# mean of control group
mu1 <- 0
# mean of alternative in treatment group
mu2 <- 2

non_null_idx <- sample(m, m*(1 - pi0))
mu <- rep(mu1, m)
mu[non_null_idx] <- mu2
sigma1 <- 1
sigma2 <- 1
control <- NULL
sigma <- rep(sigma1, m)
sigma[non_null_idx] <- sigma2
treatment <- NULL

for (i in 1:n) {
  control <- cbind(control, matrix(rnorm(m, mu1, sigma1), ncol = 1))
  treatment <- cbind(treatment, matrix(rnorm(m, mean = mu, sd = sigma), ncol = 1))
}

dat <- cbind(control, treatment)
perms <- combn(seq(1, ncol(dat)), n)
perms <- perms[, seq_len(ncol(perms)/2)]
```

```{r t comparison, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
stat <- matrix(0, nrow = ifelse(ncol(perms) > 50, 10, ncol(perms))*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")

idx <- which(apply(perms, 2, FUN = function(x) sum(x %in% (1:n)) == floor(n/2)))[1:10]
i <- 1
for (j in ifelse(ncol(perms) > 50, idx, (1:ncol(perms)))) {
  
  gpa <- dat[,  perms[, j]]
  gpb <- dat[, -perms[, j]]
  
  # test statistic
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)

  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  i <- i + 1
  
}
## Drawn a sample from null as comparison
t_sam <- rt(m, df = 2*n - 2)
t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)

plot_stat <- rbind.data.frame(stat[1: (2*m), ], t_sam)

# Density plot of test statistics
dens_plot <- ggplot(data = plot_stat, aes(stat, color = perm)) + geom_density() + xlim(-10, 10) 
dens_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))


## Two-sided p-value
p_val <- cbind.data.frame(2*(1 - pt(abs(plot_stat$stat), df = 2*n - 2)), plot_stat$perm)
colnames(p_val) <- c("p_val", "perm")

# Histogram of p-values
p_plot <- ggplot(data = p_val, aes(p_val, color = perm)) + geom_density()
p_plot + scale_color_discrete(name="Source", breaks=c("1", "2", "t"), labels=c("Observed", "Permuted", "Drawn from null"))
```

```{r t experiment, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
stat <- matrix(0, nrow = ncol(perms)*m, ncol = 2)
stat <- data.frame(stat)
colnames(stat) <- c("stat", "perm")
denominator <- NULL
for (i in 1:ncol(perms)) {
  
  gpa <- dat[,  perms[, i]]
  gpb <- dat[, -perms[, i]]
  
  # t
  stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)
  # if (i == 1) {
  #   stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)
  #   denominator <- sqrt(apply(gpa, 1, var)/n + apply(gpb, 1, var)/n)
  # } else {
  #   stat[(i - 1)*m + (1:m), 1] <- (apply(gpa, 1, mean) - apply(gpb, 1, mean))/denominator
  # }
  stat[(i - 1)*m + (1:m), 2] <- rep(as.character(i), m)
  
}
library(ggplot2)

### Find a t distribution - null case
# t_sam <- rt(m, df = 2*n - 2)
# t_sam <- cbind.data.frame(t_sam, rep('t', m))
# colnames(t_sam) <- colnames(stat)
# t_sam <- rbind(stat[1:(2*m), ], t_sam)

### Find a t distribution - alternative case
k <- 2
t_sam <- rt(m, df = 2*n - 2, ncp = -mu2*sqrt(n)/(sqrt(2)*sigma2))

### 
t_1 <- rt(m, df = 2*(n - k) - 2, ncp = -mu2*sqrt(n - k)/sqrt(2*sigma2))
t_2 <- rt(m, df = 2*k - 2, ncp = -mu2*sqrt(k)/sqrt(2*sigma2))
beta_1 <- rbeta(m, n - k - 1, k - 1)
# beta_2 <- rbeta(m, k - 1, n - k - 1)
beta_2 <- 1 - beta_1

t_sam <- t_1*sqrt((n - k)/n*beta_1) - t_2*sqrt(k/n*beta_2)
# t_sam <- t_1*sqrt((n - k)*(n - 1)/n/(n - k - 1)*beta_1) - t_2*sqrt(k*(n - 1)/n/(k - 1)*beta_2)

t_sam <- cbind.data.frame(t_sam, rep('t', m))
colnames(t_sam) <- colnames(stat)
# t_sam <- rbind(stat[1:(2*m), ], t_sam)
t_sam <- rbind(stat[c(1:m, (9*m + 1):(10*m)), ], t_sam)

#ggplot(data = stat, aes(stat, color = perm, lty = perm)) + geom_density() + xlim(-20, 5)

#ggplot(data = t_sam, aes(stat, color = perm, lty = perm)) + geom_density() + xlim(-20, 10)## + geom_vline(xintercept = -4)
```

